{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "colab-badge"
      },
      "source": [
    "<a href=\"https://colab.research.google.com/github/BhaveshGoswami11/Generative-AI/blob/main/Generative_AI_chatbot_using_LLM_from_Hugging_Face.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Simple Chatbot Tutorial - Using Local Models (No RAG, No API)\n",
    "\n",
    "**Quick Start**: Click the \"Open in Colab\" button above to run this notebook in Google Colab!\n",
    "\n",
    "---"
]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQ39geq4cQ0z"
      },
      "source": [
        "## 1) Introduction & Objectives\n",
        "\n",
        "### What is a Simple Chatbot?\n",
        "A simple chatbot uses a pre-trained language model that runs locally without needing external APIs or retrieval systems. Unlike RAG chatbots that search external knowledge bases, these chatbots rely entirely on the knowledge encoded in the model during training.\n",
        "\n",
        "### Key Differences from RAG:\n",
        "- **No external knowledge**: Answers come only from the model's training data\n",
        "- **No API costs**: Uses free, open-source models from Hugging Face\n",
        "- **Faster setup**: No need to create and maintain a vector database\n",
        "- **Limited knowledge**: Cannot access information after the model's training cutoff date\n",
        "- **Risk of hallucinations**: May generate incorrect information without source verification\n",
        "\n",
        "### Use Cases:\n",
        "- Simple Q&A bots for general knowledge\n",
        "- Conversational interfaces for learning\n",
        "- Prototyping chatbot interfaces\n",
        "- Understanding basic LLM behavior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5seqbjGAcQ02"
      },
      "source": [
        "## 2) Install Dependencies\n",
        "\n",
        "We'll use Hugging Face's Transformers library to load a pre-trained conversational model.\n",
        "\n",
        "### Libraries and their purposes:\n",
        "- **transformers** \u2192 Provides access to thousands of pre-trained models from Hugging Face\n",
        "- **torch** \u2192 PyTorch framework needed to run neural networks\n",
        "- **ipywidgets** \u2192 Creates interactive UI elements in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpowGFEscQ02"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch ipywidgets accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrJEYxvwcQ04"
      },
      "source": [
        "## 3) Load a Pre-trained Model\n",
        "\n",
        "We'll use **TinyLlama-1.1B-Chat** - a small but capable model that runs well in Colab.\n",
        "\n",
        "### Why TinyLlama?\n",
        "- Small size (1.1B parameters) - runs fast in free Colab\n",
        "- Designed for conversations\n",
        "- No API key required\n",
        "- Good balance of quality and speed\n",
        "\n",
        "### Key Concepts:\n",
        "- **Tokenizer**: Converts text into numbers the model understands\n",
        "- **Model**: The neural network that generates responses\n",
        "- **Generation settings**: Control how creative or focused the responses are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhWYsIBKcQ04"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "print(\"\ud83d\udd04 Loading model...\")\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # Fast & good quality\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"\u2705 Model loaded: {model_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTviLYRLcQ04"
      },
      "source": [
        "## 4) Create Chat Function\n",
        "\n",
        "This function handles the conversation logic:\n",
        "\n",
        "### How it works:\n",
        "1. **Encode input**: Convert user's text into tokens\n",
        "2. **Generate response**: Model predicts the next tokens\n",
        "3. **Decode output**: Convert tokens back to readable text\n",
        "4. **Maintain context**: Keep conversation history for coherent multi-turn chats\n",
        "\n",
        "### Parameters explained:\n",
        "- **max_length**: Maximum total tokens (input + output)\n",
        "- **temperature**: Controls randomness (0.7 = balanced, higher = more creative)\n",
        "- **top_p**: Nucleus sampling - considers top 90% probable words\n",
        "- **do_sample**: Enables random sampling for varied responses\n",
        "- **pad_token_id**: Tells model how to handle padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDBu7aQ9cQ05"
      },
      "outputs": [],
      "source": [
        "def chat_with_bot(user_input, conversation_history):\n",
        "\n",
        "    # Build conversation context\n",
        "    prompt = \"\"\n",
        "    for turn in conversation_history[-6:]:  # Keep last 6 turns for context\n",
        "        if turn['role'] == 'user':\n",
        "            prompt += f\"User: {turn['text']}\\n\"\n",
        "        else:\n",
        "            prompt += f\"Assistant: {turn['text']}\\n\"\n",
        "\n",
        "    prompt += f\"User: {user_input}\\nAssistant:\"\n",
        "\n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate with better parameters\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=200,  # Limit response length\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.2  # Reduce repetition\n",
        "        )\n",
        "\n",
        "    # Decode only the new tokens\n",
        "    response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "    # Clean up response\n",
        "    response = response.split(\"User:\")[0].strip()  # Remove any follow-up prompts\n",
        "    response = response.split(\"\\n\\n\")[0].strip()   # Take first paragraph\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LQUP8JVcQ05"
      },
      "source": [
        "## 5) Test the Chatbot (Simple Version)\n",
        "\n",
        "Before building the UI, let's test the chatbot with a few example questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YuterzCcQ05"
      },
      "outputs": [],
      "source": [
        "# Initialize conversation\n",
        "chat_history = [] # Initialize as an empty list\n",
        "\n",
        "# Test conversation\n",
        "print(\"=== Testing Chatbot ===\\n\")\n",
        "\n",
        "test_questions = [\n",
        "    \"Hello! How are you?\",\n",
        "    \"What do you like to talk about?\",\n",
        "    \"Tell me a fun fact\"\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"You: {question}\")\n",
        "    response = chat_with_bot(question, chat_history)\n",
        "    print(f\"Bot: {response}\\n\")\n",
        "\n",
        "print(\"\u2705 Test complete! Now let's build the interactive UI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b4KWzeRcQ06"
      },
      "source": [
        "## 6) Interactive Chat UI\n",
        "\n",
        "Now we'll create a user-friendly interface similar to the RAG chatbot, but simpler since we don't need retrieval.\n",
        "\n",
        "### UI Components:\n",
        "- **Input box**: Where users type their messages\n",
        "- **Send button**: Sends the message to the bot\n",
        "- **Clear button**: Resets the conversation\n",
        "- **Quit button**: Closes the chat interface\n",
        "- **Chat display**: Shows the conversation history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7O-efctYcQ06"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "input_box = widgets.Text(\n",
        "    placeholder='Type your message...',\n",
        "    description='You:',\n",
        "    layout=widgets.Layout(width='80%')\n",
        ")\n",
        "send_button = widgets.Button(description='Send', button_style='primary')\n",
        "clear_button = widgets.Button(description='Clear', button_style='warning')\n",
        "quit_button = widgets.Button(description='Quit', button_style='danger')\n",
        "output = widgets.HTML(value='')\n",
        "\n",
        "conversation_history = []\n",
        "\n",
        "def render_chat():\n",
        "    html = '<div style=\"max-height: 400px; overflow-y: auto; border: 1px solid #ccc; padding: 10px; border-radius: 5px; background-color: #f9f9f9;\">'\n",
        "\n",
        "    if not conversation_history:\n",
        "        html += '<p style=\"color: #666; text-align: center;\">Start chatting below! \ud83d\udc47</p>'\n",
        "\n",
        "    for turn in conversation_history:\n",
        "        role = turn['role']\n",
        "        text = turn['text'].replace('\\n', '<br>')\n",
        "\n",
        "        if role == 'user':\n",
        "            html += f'''\n",
        "            <div style=\"margin: 10px 0; text-align: right;\">\n",
        "                <span style=\"background-color: #007bff; color: white; padding: 10px 15px; border-radius: 18px; display: inline-block; max-width: 70%; box-shadow: 0 2px 5px rgba(0,0,0,0.1);\">\n",
        "                    {text}\n",
        "                </span>\n",
        "            </div>\n",
        "            '''\n",
        "        else:\n",
        "            html += f'''\n",
        "            <div style=\"margin: 10px 0;\">\n",
        "                <span style=\"background-color: #ffffff; color: black; padding: 10px 15px; border-radius: 18px; display: inline-block; max-width: 70%; box-shadow: 0 2px 5px rgba(0,0,0,0.1); border: 1px solid #e0e0e0;\">\n",
        "                    {text}\n",
        "                </span>\n",
        "            </div>\n",
        "            '''\n",
        "\n",
        "    html += '</div>'\n",
        "    output.value = html\n",
        "\n",
        "def on_send_clicked(b):\n",
        "    user_message = input_box.value.strip()\n",
        "    if not user_message:\n",
        "        return\n",
        "\n",
        "    # Add user message\n",
        "    conversation_history.append({'role': 'user', 'text': user_message})\n",
        "    render_chat()\n",
        "    input_box.value = ''\n",
        "\n",
        "    # Show thinking indicator\n",
        "    conversation_history.append({'role': 'bot', 'text': '\ud83d\udcad Thinking...'})\n",
        "    render_chat()\n",
        "\n",
        "    try:\n",
        "        # Get bot response\n",
        "        bot_response = chat_with_bot(user_message, conversation_history[:-1])\n",
        "\n",
        "        # Remove thinking indicator and add real response\n",
        "        conversation_history.pop()\n",
        "        conversation_history.append({'role': 'bot', 'text': bot_response})\n",
        "        render_chat()\n",
        "    except Exception as e:\n",
        "        conversation_history.pop()\n",
        "        conversation_history.append({'role': 'bot', 'text': f'\u274c Error: {str(e)}'})\n",
        "        render_chat()\n",
        "\n",
        "def on_clear_clicked(b):\n",
        "    conversation_history.clear()\n",
        "    render_chat()\n",
        "\n",
        "def on_quit_clicked(b):\n",
        "    clear_output(wait=True)\n",
        "    print(\"Chat closed. Re-run this cell to restart.\")\n",
        "\n",
        "send_button.on_click(on_send_clicked)\n",
        "clear_button.on_click(on_clear_clicked)\n",
        "quit_button.on_click(on_quit_clicked)\n",
        "\n",
        "ui = widgets.VBox([\n",
        "    widgets.HTML(f'<h3>\ud83e\udd16 Improved Chatbot</h3><p style=\"color: #666;\">Using: {model_name}</p>'),\n",
        "    output,\n",
        "    input_box,\n",
        "    widgets.HBox([send_button, clear_button, quit_button])\n",
        "])\n",
        "\n",
        "display(ui)\n",
        "render_chat()\n",
        "print(\"\ud83d\udcac Chat ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bkV8pHFcQ06"
      },
      "source": [
        "## 7) Comparison: RAG vs Simple Chatbot\n",
        "\n",
        "### RAG Chatbot (What we built before):\n",
        "\u2705 Accesses external knowledge sources  \n",
        "\u2705 Can answer domain-specific questions accurately  \n",
        "\u2705 Provides source citations  \n",
        "\u2705 Stays up-to-date with current information  \n",
        "\u274c Requires API keys and costs money  \n",
        "\u274c Slower due to retrieval step  \n",
        "\u274c More complex setup  \n",
        "\n",
        "### Simple Chatbot (What we built now):\n",
        "\u2705 No API costs - completely free  \n",
        "\u2705 Simpler setup  \n",
        "\u2705 Good for general conversation  \n",
        "\u274c Limited to training data knowledge  \n",
        "\u274c Can't access specific/recent information  \n",
        "\u274c No source verification  \n",
        "\u274c Higher risk of hallucinations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs9q-C8ncQ07"
      },
      "source": [
        "## 8) Key Takeaways\n",
        "\n",
        "Students should understand:\n",
        "\n",
        "1. **Simple chatbots** are good for general conversation but lack domain expertise\n",
        "2. **RAG chatbots** excel when specific, accurate information is needed\n",
        "3. **API-based models** (like GPT-4) are more powerful but cost money\n",
        "4. **Open-source models** are free but have limitations\n",
        "5. **Trade-offs** exist between cost, speed, accuracy, and complexity\n",
        "\n",
        "### When to use each approach:\n",
        "- **Simple Chatbot**: Personal projects, learning, prototyping, general chat\n",
        "- **RAG Chatbot**: Business applications, customer support, knowledge bases\n",
        "- **No Chatbot Needed**: When simple rules or search would work better!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
