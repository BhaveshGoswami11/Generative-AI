{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BhaveshGoswami11/Generative-AI/blob/main/RAG_Chatbot_MS_ACS_cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a4dfca4",
      "metadata": {
        "id": "1a4dfca4"
      },
      "source": [
        "#1) Introduction & Objectives\n",
        "\n",
        "## What is RAG (Retrieval-Augmented Generation)?\n",
        "\n",
        "**RAG** is a technique that combines **retrieval of relevant information** with **generation of text** using a large language model (LLM) like GPT. Instead of relying solely on the knowledge encoded in the model, RAG allows the model to **access external sources dynamically** to provide more accurate, up-to-date, or domain-specific answers.  \n",
        "\n",
        "**How it works (high-level):**  \n",
        "1. **Retrieve:** When a user asks a question, the system searches a **knowledge base** (e.g., documents, web pages) to find the most relevant chunks of information.  \n",
        "2. **Augment:** These retrieved documents are added as **context** to the input of the language model.  \n",
        "3. **Generate:** The LLM then uses both its internal knowledge and the retrieved context to produce a response.  \n",
        "\n",
        "**Advantages of RAG:**  \n",
        "- Reduces hallucinations (wrong answers) by grounding responses in real sources.  \n",
        "- Keeps the model up-to-date without retraining.  \n",
        "- Supports specialized domains like medical, legal, or academic knowledge.  \n",
        "\n",
        "---\n",
        "\n",
        "# Why combine OpenAI GPT with Pinecone vector DB for knowledge retrieval?\n",
        "\n",
        "OpenAI GPT alone can generate fluent text, but it **cannot search through large external datasets efficiently**. To solve this:  \n",
        "\n",
        "1. **Pinecone Vector Database** stores **embeddings** of documents (numerical representations of text).  \n",
        "2. When a query is made, it is **converted into an embedding**.  \n",
        "3. **Pinecone finds the most similar documents** using a similarity metric (e.g., cosine similarity).  \n",
        "4. These retrieved documents are fed into GPT as **context**, enabling it to answer questions with **relevant external knowledge**.  \n",
        "\n",
        "**Benefits of this combination:**  \n",
        "- **Scalable retrieval:** Pinecone can store millions of documents efficiently.  \n",
        "- **Dynamic knowledge:** GPT can provide natural language answers without needing to memorize every fact.  \n",
        "- **Accurate responses:** Retrieval ensures the model’s answers are grounded in real data rather than only its training knowledge.  \n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24b60f6",
      "metadata": {
        "id": "c24b60f6"
      },
      "source": [
        "# 2) Install Dependencies\n",
        "\n",
        "In this step, we install all the Python libraries required to build our RAG Chatbot.  \n",
        "\n",
        "### **Libraries and their purposes**\n",
        "\n",
        "- **`langchain`** → A framework to build applications using Large Language Models (LLMs). It provides tools for chaining prompts, handling context, and integrating with external data sources.  \n",
        "\n",
        "- **`langchain-openai`** → Provides connectors to OpenAI GPT models so that we can easily send prompts and receive responses from GPT.  \n",
        "\n",
        "- **`langchain-pinecone`** → Integrates LangChain with the Pinecone vector database. It allows us to store, search, and retrieve vector embeddings efficiently for retrieval-augmented generation.  \n",
        "\n",
        "- **`ipywidgets`** → Enables interactive user interfaces in Google Colab notebooks. We will use it to create input boxes, buttons, and display chat outputs dynamically.  \n",
        "\n",
        "- **`tqdm`** → Provides progress bars for loops. This is helpful when embedding or indexing many documents so students can see progress in real-time.  \n",
        "\n",
        "**Purpose:**  \n",
        "Installing these packages ensures students understand not only **how to use them**, but also **why each is necessary** in a RAG workflow.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11c8f226",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11c8f226",
        "outputId": "690a9825-1c5b-4064-fbfd-8ae6aadf75ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.6/587.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.3/259.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain-openai langchain-pinecone langchain-community pinecone-client tqdm ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU \\\n",
        "    langchain==0.3.23 \\\n",
        "    langchain-community==0.3.21 \\\n",
        "    langchain-pinecone==0.2.5 \\\n",
        "    langchain-openai==0.3.12 \\\n",
        "    datasets==3.5.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5h__CeEluYE",
        "outputId": "987dcf22-6700-4816-f2ae-806e864518e0"
      },
      "id": "W5h__CeEluYE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: pinecone 6.0.2 does not provide the extra 'async'\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.9/421.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.6/249.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce7aaf4",
      "metadata": {
        "id": "3ce7aaf4"
      },
      "source": [
        "## 3) API Key Setup\n",
        "\n",
        "In this step, we configure the API keys required to use OpenAI GPT models and Pinecone vector database.\n",
        "\n",
        "### **Why API keys are needed**\n",
        "\n",
        "- **OpenAI API key** → Allows access to OpenAI models (like GPT-4o-mini) to generate answers based on prompts.\n",
        "- **Pinecone API key** → Provides access to Pinecone’s vector database for storing and retrieving embeddings of documents.\n",
        "\n",
        "### **Security note**\n",
        "\n",
        "- API keys are sensitive information.  \n",
        "- We set them as **environment variables** (`os.environ`) so they are **not printed in the notebook output**.  \n",
        "- This prevents accidental exposure if you share your notebook publicly.\n",
        "\n",
        "### **How to use**\n",
        "\n",
        "1. Replace the placeholder strings with your own API keys:\n",
        "```python\n",
        "OPENAI_API_KEY = \"your_openai_api_key_here\"\n",
        "PINECONE_API_KEY = \"your_pinecone_api_key_here\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f284310",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f284310",
        "outputId": "252fded5-df10-4fad-8e57-385e5b4b0e98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ API keys configured (not displayed).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# === Edit these two variables only ===\n",
        "OPENAI_API_KEY = \"your_openai_api_key_here\"\n",
        "PINECONE_API_KEY = \"your_pinecone_api_key_here\"\n",
        "\n",
        "# Set environment variables (not printed)\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "print(\"✅ API keys configured (not displayed).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b761fe0",
      "metadata": {
        "id": "2b761fe0"
      },
      "source": [
        "## 4) Helper Functions: Load, Chunk, Initialize, Create Knowledge Base\n",
        "\n",
        "In this section, we define the **core helper functions** that are essential for building a RAG Chatbot. Each function has a specific role in preparing and indexing knowledge from web URLs.\n",
        "\n",
        "### **1. `load_web_content(urls)`**\n",
        "- Scrapes web pages from a list of URLs.\n",
        "- Returns a list of **Document objects**, each containing the text content and metadata.\n",
        "- Metadata includes the **source URL**, which is important for traceability. This helps later show **where each answer comes from**.\n",
        "\n",
        "### **2. `chunk_documents(documents)`**\n",
        "- Splits long text documents into smaller chunks.\n",
        "- Why? LLMs like GPT have a **maximum input token limit**, and smaller chunks improve the quality of retrieval and relevance.\n",
        "- Parameters:\n",
        "  - `chunk_size` → maximum number of characters per chunk.\n",
        "  - `chunk_overlap` → number of characters overlapping between chunks to preserve context.\n",
        "\n",
        "### **3. `initialize_pinecone_index(api_key)`**\n",
        "- Creates a **vector index** in Pinecone if it doesn’t exist.\n",
        "- The index stores **document embeddings**, enabling efficient similarity search.\n",
        "- Cosine similarity is used to find the most relevant chunks for a query.\n",
        "\n",
        "### **4. `create_knowledge_base(urls, api_key)`**\n",
        "- Combines all steps to create a retrievable knowledge base:\n",
        "  1. Load web content using `load_web_content`.\n",
        "  2. Split documents into chunks using `chunk_documents`.\n",
        "  3. Generate embeddings for each chunk using **OpenAI embeddings**.\n",
        "  4. Store embeddings in **Pinecone vector DB**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d965ebb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d965ebb",
        "outputId": "459a6649-90e9-4f99-97b8-4bacd548691f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain.schema import SystemMessage, HumanMessage\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec, CloudProvider, AwsRegion, Metric\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def load_web_content(urls):\n",
        "    all_documents = []\n",
        "    for url in urls:\n",
        "        try:\n",
        "            print(f\"Loading content from: {url}\")\n",
        "            loader = WebBaseLoader(url)\n",
        "            documents = loader.load()\n",
        "            for doc in documents:\n",
        "                doc.metadata['source'] = url\n",
        "            all_documents.extend(documents)\n",
        "            print(f\"✓ Loaded {len(documents)} document(s) from {url}\")\n",
        "        except Exception as e:\n",
        "            print(f\"✗ Error loading {url}: {str(e)}\")\n",
        "    return all_documents\n",
        "\n",
        "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)\n",
        "    chunks = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(chunks)} chunks\")\n",
        "    return chunks\n",
        "\n",
        "def initialize_pinecone_index(api_key, index_name=\"rag-pipeline\"):\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    if not pc.has_index(name=index_name):\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            metric=Metric.COSINE,\n",
        "            dimension=1536,\n",
        "            spec=ServerlessSpec(cloud=CloudProvider.AWS, region=AwsRegion.US_EAST_1)\n",
        "        )\n",
        "        print(f\"Created new index: {index_name}\")\n",
        "    else:\n",
        "        print(f\"Using existing index: {index_name}\")\n",
        "    return pc.Index(name=index_name)\n",
        "\n",
        "def create_knowledge_base(urls, api_key, index_name=\"rag-pipeline\"):\n",
        "    documents = load_web_content(urls)\n",
        "    if not documents:\n",
        "        raise ValueError(\"No documents were loaded successfully\")\n",
        "    chunks = chunk_documents(documents)\n",
        "    embed_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    index = initialize_pinecone_index(api_key, index_name)\n",
        "    print(\"Embedding and indexing documents...\")\n",
        "    batch_size = 100\n",
        "    for i in tqdm(range(0, len(chunks), batch_size)):\n",
        "        i_end = min(len(chunks), i + batch_size)\n",
        "        batch = chunks[i:i_end]\n",
        "        ids = [f\"doc-{i+j}\" for j in range(len(batch))]\n",
        "        texts = [doc.page_content for doc in batch]\n",
        "        embeds = embed_model.embed_documents(texts)\n",
        "        metadata = [{'text': doc.page_content, 'source': doc.metadata.get('source', 'unknown')} for doc in batch]\n",
        "        index.upsert(vectors=zip(ids, embeds, metadata))\n",
        "    print(f\"✓ Indexed {len(chunks)} chunks\")\n",
        "    return PineconeVectorStore(index=index, embedding=embed_model, text_key=\"text\")\n",
        "\n",
        "def augment_prompt(query, vectorstore, k=3):\n",
        "    results = vectorstore.similarity_search(query, k=k)\n",
        "    source_knowledge = \"\\n\\n\".join([x.page_content for x in results])\n",
        "    sources = [x.metadata.get('source', 'unknown') for x in results]\n",
        "    augmented_prompt = f\"\"\"Using the contexts below, answer the query.\n",
        "\n",
        "Contexts:\n",
        "{source_knowledge}\n",
        "\n",
        "Query: {query}\"\"\"\n",
        "    return augmented_prompt, sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9789012d",
      "metadata": {
        "id": "9789012d"
      },
      "source": [
        "## 5) Create Knowledge Base (Run Once)\n",
        "\n",
        "In this step, we **load, embed, and index all web pages** into Pinecone to create a knowledge base.  \n",
        "\n",
        "### **Key Points for Students**\n",
        "\n",
        "- **Indexing is expensive:**  \n",
        "  - Embedding each chunk of text requires API calls to OpenAI.  \n",
        "  - Uploading embeddings to Pinecone also takes time.  \n",
        "  - Therefore, we **run this step only once**. After the knowledge base is created, we can reuse it for multiple queries.\n",
        "\n",
        "- **Vectorstore creation:**  \n",
        "  - The `create_knowledge_base(urls, api_key)` function returns a **vectorstore object**, which contains the embeddings and Pinecone index.  \n",
        "  - This is stored in a variable named `vectorstore` for reuse.  \n",
        "  - Example:  \n",
        "    ```python\n",
        "    vectorstore = create_knowledge_base(urls, api_key=PINECONE_API_KEY, index_name=\"rag-pipeline\")\n",
        "    ```\n",
        "  - Once created, you can use `vectorstore` for **retrieval without re-indexing**.\n",
        "\n",
        "- **Editing URLs:**  \n",
        "  - Students can **add or remove web pages** in the `urls` list to change the knowledge base content.  \n",
        "  - Example:  \n",
        "    ```python\n",
        "    urls = [\n",
        "        \"https://www.nwmissouri.edu/csis/msacs/\",\n",
        "        \"https://www.nwmissouri.edu/csis/msacs/about.htm\",\n",
        "        # Add your own URLs here\n",
        "    ]\n",
        "    ```\n",
        "\n",
        "**Purpose:**  \n",
        "This step teaches students how **raw web content is converted into a retrievable knowledge base** and emphasizes the cost and time implications of embedding and indexing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49c31a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327,
          "referenced_widgets": [
            "54a53b4c49d94c8e837f4187010a44aa",
            "de291470cab2411ca91a7fdc06b068c3",
            "87a463a150764ee4bed00dcf2f7a94fe",
            "8fd47ae633c24bc3a041b7d15908a913",
            "47fe5161b1db4a87a9ba7fe5cf64eb9b",
            "800e66c501a14ef58a695ba35d09c729",
            "94f3a552ec7d454398a5b8427ede9256",
            "d3fee2120f9f4b7987ddac8a9872384b",
            "cff821ac43224f62bb792e8fdb903353",
            "00479477d4b84a6c818e02d938737b14",
            "dae739d9e31c4942bce35d1701aa7ea2"
          ]
        },
        "id": "b49c31a6",
        "outputId": "096afcf3-1900-461c-8db3-92b56280fa57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating knowledge base from web URLs...\n",
            "Loading content from: https://www.nwmissouri.edu/csis/msacs/\n",
            "✓ Loaded 1 document(s) from https://www.nwmissouri.edu/csis/msacs/\n",
            "Loading content from: https://www.nwmissouri.edu/csis/msacs/about.htm\n",
            "✓ Loaded 1 document(s) from https://www.nwmissouri.edu/csis/msacs/about.htm\n",
            "Loading content from: https://www.nwmissouri.edu/academics/graduate/masters/applied-computer-science.htm\n",
            "✓ Loaded 1 document(s) from https://www.nwmissouri.edu/academics/graduate/masters/applied-computer-science.htm\n",
            "Loading content from: https://www.nwmissouri.edu/csis/msacs/apply/index.htm\n",
            "✓ Loaded 1 document(s) from https://www.nwmissouri.edu/csis/msacs/apply/index.htm\n",
            "Loading content from: https://www.nwmissouri.edu/csis/msacs/courses.htm\n",
            "✓ Loaded 1 document(s) from https://www.nwmissouri.edu/csis/msacs/courses.htm\n",
            "Split into 26 chunks\n",
            "Using existing index: rag-pipeline\n",
            "Embedding and indexing documents...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Indexed 26 chunks\n",
            "Vectorstore variable is ready for queries.\n"
          ]
        }
      ],
      "source": [
        "# Example URLs (edit if needed)\n",
        "urls = [\n",
        "    \"https://www.nwmissouri.edu/csis/msacs/\",\n",
        "    \"https://www.nwmissouri.edu/csis/msacs/about.htm\",\n",
        "    \"https://www.nwmissouri.edu/academics/graduate/masters/applied-computer-science.htm\",\n",
        "    \"https://www.nwmissouri.edu/csis/msacs/apply/index.htm\",\n",
        "    \"https://www.nwmissouri.edu/csis/msacs/courses.htm\"\n",
        "]\n",
        "\n",
        "# Create knowledge base and store in `vectorstore` variable for reuse\n",
        "print(\"Creating knowledge base from web URLs...\")\n",
        "vectorstore = create_knowledge_base(urls, api_key=os.environ.get(\"PINECONE_API_KEY\"), index_name=\"rag-pipeline\")\n",
        "print('Vectorstore variable is ready for queries.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79df25cd",
      "metadata": {
        "id": "79df25cd"
      },
      "source": [
        "## 6) Reusable Query Cell\n",
        "### **How RAG retrieval works**\n",
        "\n",
        "1. **Query:** Student enters a question.  \n",
        "2. **Similarity search:** The `vectorstore.similarity_search(query, k=3)` retrieves the **top 3 most relevant document chunks** from the knowledge base.  \n",
        "   - `k=3` means we consider **the 3 most similar chunks** for context.  \n",
        "3. **Augment prompt:** These retrieved chunks are added to the prompt, providing the LLM with relevant context.  \n",
        "4. **LLM generates answer:** GPT uses the augmented prompt to produce a response.  \n",
        "\n",
        "### **Key teaching points**\n",
        "\n",
        "- **Reusable:** This cell can be run **multiple times** for different queries without re-indexing the documents.  \n",
        "- **Dynamic:** Each query retrieves fresh context from the vectorstore.  \n",
        "- Students can **experiment with `k`** to see how increasing or decreasing the number of chunks affects answer quality.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d22f89f5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d22f89f5",
        "outputId": "3ff8c685-2ff9-47b4-f2be-661a4abbc78a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1331979542.py:10: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chat([system_msg, HumanMessage(content=augmented_query)])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== QUESTION ===\n",
            "What courses are offered in the MSACS program?\n",
            "\n",
            "=== ANSWER ===\n",
            "The M.S. Applied Computer Science (MSACS) program at Northwest offers a variety of computer science courses. Each course is linked to a detailed syllabus that includes prerequisite information and the semesters they are offered. For specific courses, including popular electives, you can check the program advisor for the most up-to-date information or visit the detailed course descriptions page at the provided link: [MSACS Course Descriptions](https://www.nwmissouri.edu/csis/msacs/courses.htm).\n",
            "\n",
            "=== SOURCES ===\n",
            "https://www.nwmissouri.edu/csis/msacs/FAQs.htm, https://www.nwmissouri.edu/csis/msacs/about.htm, https://www.nwmissouri.edu/csis/msacs/courses.htm\n"
          ]
        }
      ],
      "source": [
        "# Example: change this query and re-run as many times as you want\n",
        "query = \"What courses are offered in the MSACS program?\"\n",
        "\n",
        "# Augment with context and call the chat model\n",
        "augmented_query, sources = augment_prompt(query, vectorstore, k=3)\n",
        "\n",
        "chat = ChatOpenAI(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
        "system_msg = SystemMessage(content=\"You are a helpful assistant that answers questions based on the provided context.\")\n",
        "\n",
        "response = chat([system_msg, HumanMessage(content=augmented_query)])\n",
        "print(\"\\n=== QUESTION ===\")\n",
        "print(query)\n",
        "print(\"\\n=== ANSWER ===\")\n",
        "print(response.content)\n",
        "print(\"\\n=== SOURCES ===\")\n",
        "print(\", \".join(set(sources)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c8dd15",
      "metadata": {
        "id": "72c8dd15"
      },
      "source": [
        "## 7) Interactive Chat UI\n",
        "\n",
        "### **UI Components**\n",
        "\n",
        "- **Input box:** Where students type their questions.  \n",
        "- **Buttons:**  \n",
        "  - **Ask** → sends the query to the RAG Chatbot.  \n",
        "  - **Clear** → clears the conversation history.  \n",
        "  - **Quit** → stops the interaction.  \n",
        "- **Conversation history list:** Stores and displays all previous user queries and GPT answers.  \n",
        "\n",
        "### **How it works**\n",
        "\n",
        "1. User enters a question in the input box.  \n",
        "2. Upon clicking **Ask**, the query is sent through the **retrieval + augmentation process**.  \n",
        "3. GPT generates an answer using the retrieved context.  \n",
        "4. The question and answer are displayed dynamically in a **simple chat bubble interface** using HTML.  \n",
        "5. Buttons allow students to **manage the conversation** interactively.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acabacb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74,
          "referenced_widgets": [
            "2048c97efa844bce9601b3255c6e86eb",
            "f34eb38c342747f2804e6d689388b94e",
            "f814c3114bd846cbb7e38f8e6c296b71",
            "b1fa01d416a0424dab11e091ec3ebff2",
            "c0571e233ada4233917652561546db3f",
            "4bae57876b064b73bee7ae84cbd68fb5",
            "ecf488cf897942269826a89710a2a9b1",
            "8d32c499077141d5bf374830cd73a067",
            "5b1d0f49288b400498f5a251f1473fe5",
            "b17b650e658d4582a027803647b17990",
            "1c49090d5788481ea5cdc6e1a3ffca1f",
            "24d36a6f6e9147e787634d3763b6f581",
            "6a7eb5ea2c8c442bbe059a7bba1727af",
            "1bc5c627462746678df1e01e21b00712",
            "487305c6712647e8ad7a47d490581d5f",
            "f2fe7adcbac84003a9802aed85731ac5",
            "238ba2731a994d289226680b52117088"
          ]
        },
        "id": "acabacb4",
        "outputId": "b86f266e-c021-49a7-ab52-0e8d7c0c8566"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HBox(children=(Text(value='', description='Question:', layout=Layout(width='80%'), placeholder='Type your ques…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "HTML(value='', placeholder='')"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Create widgets\n",
        "input_box = widgets.Text(placeholder='Type your question here...', description='Question:', layout=widgets.Layout(width='80%'))\n",
        "ask_button = widgets.Button(description='Ask', button_style='primary')\n",
        "quit_button = widgets.Button(description='Quit', button_style='danger')\n",
        "clear_button = widgets.Button(description='Clear', button_style='warning')\n",
        "output = widgets.HTML(value='', placeholder='', description='')\n",
        "\n",
        "# Keep conversation history in a list\n",
        "conversation_history = []\n",
        "\n",
        "def render_history():\n",
        "    # Render simple HTML chat window\n",
        "    html = '<div style=\"max-height:400px; overflow:auto; border:1px solid #ddd; padding:10px; background:#f9f9f9;\">'\n",
        "    for turn in conversation_history:\n",
        "        role = turn['role']\n",
        "        text = turn['text'].replace('\\n', '<br>')\n",
        "        if role == 'user':\n",
        "            html += f\"<div style='text-align:right; margin:6px 0;'><b>You:</b> <span style='background:#dbeafe; padding:6px; border-radius:6px;'>{text}</span></div>\"\n",
        "        else:\n",
        "            html += f\"<div style='text-align:left; margin:6px 0;'><b>Bot:</b> <span style='background:#eef2ff; padding:6px; border-radius:6px;'>{text}</span></div>\"\n",
        "    html += '</div>'\n",
        "    output.value = html\n",
        "\n",
        "def on_ask_clicked(b):\n",
        "    question = input_box.value.strip()\n",
        "    if not question:\n",
        "        return\n",
        "    conversation_history.append({'role':'user','text':question})\n",
        "    render_history()\n",
        "    input_box.value = ''\n",
        "\n",
        "    # Retrieve context and get answer\n",
        "    augmented_query, sources = augment_prompt(question, vectorstore, k=3)\n",
        "    chat = ChatOpenAI(openai_api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
        "    system_msg = SystemMessage(content=\"You are a helpful assistant that answers questions based on the provided context.\")\n",
        "    resp = chat([system_msg, HumanMessage(content=augmented_query)])\n",
        "    answer = resp.content\n",
        "    conversation_history.append({'role':'bot','text':answer + \"<br><br><small><i>Sources: \" + \", \".join(set(sources)) + \"</i></small>\"})\n",
        "    render_history()\n",
        "\n",
        "def on_quit_clicked(b):\n",
        "    clear_output(wait=True)\n",
        "    print(\"Chat UI closed. You can re-run the Chat UI cell to reopen it.\")\n",
        "\n",
        "def on_clear_clicked(b):\n",
        "    conversation_history.clear()\n",
        "    render_history()\n",
        "\n",
        "ask_button.on_click(on_ask_clicked)\n",
        "quit_button.on_click(on_quit_clicked)\n",
        "clear_button.on_click(on_clear_clicked)\n",
        "\n",
        "ui = widgets.HBox([input_box, ask_button, clear_button, quit_button])\n",
        "display(ui, output)\n",
        "render_history()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}